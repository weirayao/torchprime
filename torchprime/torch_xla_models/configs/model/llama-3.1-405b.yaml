defaults:
  - _self_
  - sharding: llama-fsdp-tp
  - remat: llama-scan-offload

model_class: llama.LlamaForCausalLM
vocab_size: 128256
hidden_size: 16384
intermediate_size: 53248
num_hidden_layers: 126
num_attention_heads: 128
num_key_value_heads: 8
hidden_act: silu
max_position_embeddings: 131072
bos_token_id: 128000
eos_token_id: 128001
tokenizer_name: meta-llama/Meta-Llama-3.1-405B
initializer_range: 0.02
rms_norm_eps: 1.0e-05
attention_dropout: false
attention_bias: false
# choose attention_kernel from: [flash_attention, splash_attention, null]
attention_kernel: flash_attention
rope_theta: 500000.0
rope_scaling:
  factor: 8.0
  low_freq_factor: 1.0
  high_freq_factor: 4.0
  original_context_len: 8192
