defaults:
  - _self_  # refers to this config file
  - sharding: qwen-fsdp  # refers to sharding/qwen-fsdp.yaml
  - remat: qwen2  # refers to remat/qwe2.yaml

model_class: flex.Qwen2ForCausalLM  # Used to import the model from this class
attention_bias: false
attention_dropout: 0.0
bos_token_id: 151643
eos_token_id: 151643
pad_token_id: 151643
mask_token_id: 151669
tokenizer_name: Qwen/Qwen2.5-Coder-1.5B
head_dim: 128
hidden_act: silu
hidden_size: 1536
initializer_range: 0.02
intermediate_size: 8960
max_position_embeddings: 32768
max_window_layers: 28
num_attention_heads: 12
num_hidden_layers: 28
num_key_value_heads: 8
rms_norm_eps: 1e-06
rope_scaling: null
rope_theta: 1000000
sliding_window: 32768
tie_word_embeddings: true
torch_dtype: bfloat16
use_cache: true
use_sliding_window: false
vocab_size: 151936
# choose attention_kernel from: [flash_attention, splash_attention, null]
attention_kernel: flash_attention
# attention_kernel: null