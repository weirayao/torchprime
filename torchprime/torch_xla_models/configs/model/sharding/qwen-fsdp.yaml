# Weights
model.embed_tokens.weight: [fsdp, null]
model.layers.*.self_attn.q_proj.weight: [fsdp, null]
model.layers.*.self_attn.k_proj.weight: [null, fsdp]
model.layers.*.self_attn.v_proj.weight: [null, fsdp]
model.layers.*.self_attn.o_proj.weight: [fsdp, null]
model.layers.*.mlp.gate_proj.weight: [fsdp, null]
model.layers.*.mlp.up_proj.weight: [fsdp, null]
model.layers.*.mlp.down_proj.weight: [null, fsdp]
model.layers.*.input_layernorm.weight: [fsdp]
model.layers.*.post_attention_layernorm.weight: [fsdp]
model.norm.weight: [fsdp]
lm_head.weight: [fsdp, null]

# Activations
model.layers.*: [fsdp, null, null]
lm_head: [fsdp, null, null]
