# Weights
model.embed_tokens.weight: [fsdp, tensor]
model.layers.*.self_attn.q_proj.weight: [fsdp, tensor]
model.layers.*.self_attn.k_proj.weight: [tensor, fsdp]
model.layers.*.self_attn.v_proj.weight: [tensor, fsdp]
model.layers.*.self_attn.o_proj.weight: [fsdp, tensor]
model.layers.*.mlp.gate_proj.weight: [fsdp, tensor]
model.layers.*.mlp.up_proj.weight: [fsdp, tensor]
model.layers.*.mlp.down_proj.weight: [tensor, fsdp]
# model.layers.*.input_layernorm.weight: [fsdp]
# model.layers.*.post_attention_layernorm.weight: [fsdp]
# model.norm.weight: [fsdp]
lm_head.weight: [fsdp, tensor]

# Activations
model.layers.*: [fsdp, null, tensor]
lm_head: [fsdp, null, tensor]
