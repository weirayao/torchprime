defaults:
  - _self_  # refers to this config file
  - sharding: qwen-fsdp  # refers to sharding/llama-fsdp.yaml
  - remat: qwen  # refers to remat/llama.yaml

model_class: qwen.Qwen3ForCausalLM  # Used to import the model from this class
attention_bias: false
attention_dropout: false
bos_token_id: 151643
eos_token_id: 151645
tokenizer_name: Qwen/Qwen3-8B
head_dim: 128
hidden_act: silu
hidden_size: 4096
initializer_range: 0.02
intermediate_size: 12288
max_position_embeddings: 40960
max_window_layers: 36
num_attention_heads: 32
num_hidden_layers: 36
num_key_value_heads: 8
rms_norm_eps: 1e-06
rope_scaling: null
rope_theta: 1000000
sliding_window: null
tie_word_embeddings: false
torch_dtype: bfloat16
use_cache: true
use_sliding_window: false
vocab_size": 151936
# choose attention_kernel from: [flash_attention, splash_attention, null]
attention_kernel: flash_attention