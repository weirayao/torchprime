defaults:
  - _self_  # refers to this config file
  - sharding: mixtral-fsdp  # refers to sharding/mixtral-fsdp.yaml
  - remat: mixtral  # refers to remat/mixtral.yaml

model_class: mixtral.MixtralForCausalLM  # Used to import the model from this class
bos_token_id: 1
eos_token_id: 2
hidden_size: 4096
initializer_range: 0.02
intermediate_size: 14336
max_position_embeddings: 32768
num_attention_heads: 32
num_experts_per_tok: 2
num_hidden_layers: 32
capacity_factor: 1.25
num_key_value_heads: 8
num_local_experts: 8
rms_norm_eps: 1e-05
rope_theta: 1000000.0
router_aux_loss_coef: 0.02
vocab_size: 32000
attention_bias: false
attention_dropout: 0.0
# choose attention_kernel from: [flash_attention, splash_attention, null]
attention_kernel: flash_attention
# choose moe_implementation from: [gmm, gmm_stack, static, dropping]
moe_implementation: dropping
tokenizer_name: mistralai/Mixtral-8x7B-v0.1
