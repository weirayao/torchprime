defaults:
  - _self_  # refers to this config file

block_size: 8192
cache_dir: /tmp/

# For SFT, we typically use instruction-following datasets
# Huggingface dataset name, will OVERRIDE gcs_dataset_names if provided
dataset_name: tatsu-lab/alpaca
dataset_config_name: null

# Alternative SFT datasets (uncomment to use):
# OpenCoder datasets:
# dataset_name: OpenCoder-LLM/opc-sft-stage1
# dataset_config_name: filtered_infinity_instruct  # or largescale_diverse_instruct, realuser_instruct

# Other popular SFT datasets:
# dataset_name: databricks/databricks-dolly-15k
# dataset_config_name: null

# dataset_name: sahil2801/CodeAlpaca-20k
# dataset_config_name: null

# GCS dataset names and mixing weights for SFT datasets
gcs_dataset_names: null
weights: null

# SFT-specific configuration
sft:
  # Format of the instruction data: "alpaca", "sharegpt", "custom"
  # This determines how instruction_lengths are extracted from the data
  format: alpaca
  
  # For custom format, specify the field names
  custom_format:
    instruction_field: "instruction"
    response_field: "response"
    # Optional: field that contains system prompt
    system_field: "system"
  
  # Whether to include system prompts in the instruction part
  include_system_prompt: true
  
  # Separator between instruction and response (if not using structured format)
  instruction_response_separator: "\n\n### Response:\n"
  
  # Whether to use IterableDataset for better data distribution across processes
  # Default: false (recommended for multi-process training)
  # Set to true to use IterableDataset (may cause sharding issues)
  use_iterable_dataset: true 