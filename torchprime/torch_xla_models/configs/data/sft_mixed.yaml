defaults:
  - _self_  # refers to this config file

block_size: 8192
cache_dir: /tmp/

# For mixed SFT, we can specify multiple datasets with weights
# Multiple HuggingFace datasets with configs and weights
hf_datasets:
  - name: OpenCoder-LLM/opc-sft-stage1
    config: filtered_infinity_instruct
    weight: 0.3
  - name: OpenCoder-LLM/opc-sft-stage1
    config: largescale_diverse_instruct
    weight: 0.5
  - name: OpenCoder-LLM/opc-sft-stage1
    config: realuser_instruct
    weight: 0.2

# Legacy single dataset support (will be ignored if hf_datasets is provided)
dataset_name: null
dataset_config_name: null

# GCS dataset names and mixing weights for SFT datasets
gcs_dataset_names: null
weights: null

# SFT-specific configuration
sft:
  # Format of the instruction data: "alpaca", "sharegpt", "custom"
  # This determines how instruction_lengths are extracted from the data
  format: alpaca
  
  # For custom format, specify the field names
  custom_format:
    instruction_field: "instruction"
    response_field: "response"
    # Optional: field that contains system prompt
    system_field: "system"
  
  # Whether to include system prompts in the instruction part
  include_system_prompt: true
  
  # Separator between instruction and response (if not using structured format)
  instruction_response_separator: "\n\n### Response:\n"
  
  # Whether to use IterableDataset for better data distribution across processes
  # Default: true (recommended for multi-process training)
  # Uses native HuggingFace IterableDataset operations for efficient streaming and shuffling
  # Set to false to use regular Dataset (fallback if IterableDataset fails)
  use_iterable_dataset: true 