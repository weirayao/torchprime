{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/haolin.chen/torchprime/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/haolin.chen/torchprime/venv/lib/python3.10/site-packages/torch_xla/__init__.py:258: UserWarning: `tensorflow` can conflict with `torch-xla`. Prefer `tensorflow-cpu` when using PyTorch/XLA. To silence this warning, `pip uninstall -y tensorflow && pip install tensorflow-cpu`. If you are in a notebook environment such as Colab or Kaggle, restart your notebook runtime afterwards.\n",
      "  warnings.warn(\n",
      "WARNING:root:libtpu.so and TPU device found. Setting PJRT_DEVICE=TPU.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from datasets import load_dataset, load_from_disk, Dataset\n",
    "from datasets.distributed import split_dataset_by_node\n",
    "from transformers import AutoTokenizer\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen3-1.7B\")\n",
    "if tokenizer.mask_token is None:\n",
    "    tokenizer.add_tokens(\"<|mask|>\", special_tokens=True)\n",
    "    tokenizer.add_special_tokens(\n",
    "        {\"mask_token\": \"<|mask|>\"}, replace_additional_special_tokens=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('<|endoftext|>', '<|im_end|>')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.pad_token, tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"loubnabnl/humaneval_infilling\", name=\"HumanEval-RandomSpanInfillingLight\", split=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.map(lambda x: {\"canonical_solution_length\": len(tokenizer(x[\"canonical_solution\"])[\"input_ids\"]) })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assemble_query(x):\n",
    "    prompts = x[\"prompt\"]\n",
    "    suffixes = x[\"suffix\"]\n",
    "    canonical_solution_lengths = x[\"canonical_solution_length\"]\n",
    "    \n",
    "    queries = []\n",
    "    for prompt, suffix, length in zip(prompts, suffixes, canonical_solution_lengths):\n",
    "        num_infill_tokens = max(64, length)\n",
    "        query = prompt + tokenizer.mask_token * num_infill_tokens + suffix\n",
    "        queries.append(query)\n",
    "    \n",
    "    return {\"query\": queries}\n",
    "\n",
    "dataset = dataset.map(assemble_query, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 164/164 [00:00<00:00, 2114.17 examples/s]\n"
     ]
    }
   ],
   "source": [
    "column_names = list(dataset.features)\n",
    "tokenized_dataset = dataset.map(\n",
    "    lambda x: tokenizer(x[\"query\"]), batched=True, remove_columns=column_names\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "558"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(tokenized_dataset.map(lambda x:  {\"length\": len(x[\"input_ids\"])})[\"length\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "from typing import List\n",
      "\n",
      "\n",
      "def has_close_elements(numbers: List[float], threshold: float) -> bool:\n",
      "    \"\"\" Check if in given list of numbers, are any two numbers closer to each other than\n",
      "    given threshold.\n",
      "    >>> has_close_elements([1.0, 2.0, 3.0], 0.5)\n",
      "    False\n",
      "    >>> has_close_elements([1.0, 2.8, 3.0, 4.0, 5.0, 2.0], 0.3)\n",
      "    True\n",
      "    \"\"\"\n",
      "    for idx, elem in enumerate(numbers):\n",
      "        for idx2, elem2 in enumerate(numbers):<|mask|><|mask|><|mask|><|mask|><|mask|><|mask|><|mask|><|mask|><|mask|><|mask|><|mask|><|mask|><|mask|><|mask|><|mask|><|mask|><|mask|><|mask|><|mask|><|mask|><|mask|><|mask|><|mask|><|mask|><|mask|><|mask|><|mask|><|mask|><|mask|><|mask|><|mask|><|mask|><|mask|><|mask|><|mask|><|mask|><|mask|><|mask|><|mask|><|mask|><|mask|><|mask|><|mask|><|mask|><|mask|><|mask|><|mask|><|mask|><|mask|><|mask|><|mask|><|mask|><|mask|><|mask|><|mask|><|mask|><|mask|><|mask|><|mask|><|mask|><|mask|><|mask|><|mask|><|mask|>\n",
      "                if distance < threshold:\n",
      "                    return True\n",
      "\n",
      "    return False\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(dataset[\"query\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'openai_humaneval'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_humaneval = load_dataset(\"openai/openai_humaneval\", split=\"test\")\n",
    "dataset_humaneval.config_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'HumanEval-RandomSpanInfillingLight'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.config_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['DM_Mathematics',\n",
       " 'Falcon-refinedweb',\n",
       " 'Gutenberg',\n",
       " 'RedPajama',\n",
       " 'RedPajama_math',\n",
       " 'Redpajama-Arxiv',\n",
       " 'Wikipedia_en',\n",
       " 'c4_2023-14',\n",
       " 'cosmopedia_v2_parquet',\n",
       " 'dclm-baseline-1.0-shuffled',\n",
       " 'fineweb_edu_dedup',\n",
       " 'open-web-math',\n",
       " 'python_edu',\n",
       " 'stackv2_Python_shuffled',\n",
       " 'the-stack-v2-train-smol']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_names = [f.split(\"/\")[-1] for f in glob(\"/home/haolin.chen/sfr-text-diffusion-model-research/data/xgen_cleaned_data/*\")]\n",
    "dataset_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DM_Mathematics: ['.json'] (42 files)\n",
      "Falcon-refinedweb: ['.json'] (1251 files)\n",
      "Gutenberg: ['.json'] (110 files)\n",
      "RedPajama: ['.json'] (1005 files)\n",
      "RedPajama_math: ['.json'] (1668 files)\n",
      "Redpajama-Arxiv: ['.json'] (216 files)\n",
      "Wikipedia_en: ['.json'] (194 files)\n",
      "c4_2023-14: ['.json'] (5001 files)\n",
      "cosmopedia_v2_parquet: ['.parquet'] (104 files)\n",
      "dclm-baseline-1.0-shuffled: ['.json'] (1001 files)\n",
      "fineweb_edu_dedup: ['.parquet'] (234 files)\n",
      "open-web-math: ['.json'] (418 files)\n",
      "python_edu: ['.jsonl'] (63 files)\n",
      "stackv2_Python_shuffled: ['.json'] (513 files)\n",
      "the-stack-v2-train-smol: ['.json'] (838 files)\n"
     ]
    }
   ],
   "source": [
    "# Scan files to determine data types for each dataset\n",
    "dataset_types = {}\n",
    "\n",
    "for dataset_name in dataset_names:\n",
    "    dataset_path = f\"/home/haolin.chen/sfr-text-diffusion-model-research/data/xgen_cleaned_data/{dataset_name}\"\n",
    "    files = glob(f\"{dataset_path}/*\")\n",
    "    \n",
    "    if files:\n",
    "        # Get file extensions\n",
    "        extensions = set()\n",
    "        for file in files[:10]:  # Check first 10 files\n",
    "            if os.path.isfile(file):\n",
    "                ext = os.path.splitext(file)[1]\n",
    "                if ext:\n",
    "                    extensions.add(ext)\n",
    "        \n",
    "        print(f\"{dataset_name}: {list(extensions)} ({len(files)} files)\")\n",
    "        extension = list(extensions)[0]\n",
    "        match extension:\n",
    "            case \".jsonl\":\n",
    "                dataset_types[dataset_name] = (extension, \"json\")\n",
    "            case \".json\":\n",
    "                dataset_types[dataset_name] = (extension, \"json\")\n",
    "            case \".parquet\":\n",
    "                dataset_types[dataset_name] = (extension, \"parquet\")\n",
    "    else:\n",
    "        print(f\"{dataset_name}: No files found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python_edu IterableDataset({\n",
      "    features: Unknown,\n",
      "    n_shards: 1\n",
      "}) dict_keys(['blob_id', 'repo_name', 'path', 'length_bytes', 'score', 'int_score', 'text']) {'blob_id': '55884a59514464a78f8002779532a7eb01b8331c', 'repo_name': 'sudajzp/jzp-s-python', 'path': '/FBNQ_py/Fib_circle.py', 'length_bytes': 854, 'score': 3.84375, 'int_score': 4, 'text': \"#coding utf-8\\n'''\\n斐波那契数列-循环法\\n'''\\ndef Fib_circle():\\n    while True:   # 去掉while循环，只用for循环\\n        num_1 = 0\\n        num_2 = 1\\n        fib_array = [0] # 用于存储计算出的FB数列值\\n        m = input('你想要查找的起始项：')\\n        n = input('你想要查找的结束项：')\\n        if m.isdigit() and n.isdigit():   # 在这个实现函数中，不要进行检验。每个函数只做一个事情\\n            m = int(m) # 将输入化为整数型\\n            n = int(n)\\n            for i in range(n):\\n                num_1, num_2 = num_2, num_1 + num_2\\n                fib_array.append(num_1)\\n            print(f'你要查找的数列为{list(enumerate(fib_array[m:], m))}')\\n            break\\n        else:\\n            print('请输入有效的正整数')\\n\\nif __name__ == '__main__':\\n    Fib_circle()\\n\"}\n",
      "--------------------------------\n"
     ]
    }
   ],
   "source": [
    "datasets = {}\n",
    "for dataset_name, (extention, dataset_type) in dataset_types.items():\n",
    "    if dataset_name == \"python_edu\":\n",
    "        files = glob(f\"/home/haolin.chen/sfr-text-diffusion-model-research/data/xgen_cleaned_data/{dataset_name}/*{extention}\")\n",
    "        try:\n",
    "            datasets[dataset_name] = load_dataset(dataset_type, data_files=files[:1], streaming=True, split=\"train\")\n",
    "            iterator = iter(datasets[dataset_name])\n",
    "            row = next(iterator)\n",
    "            print(dataset_name, datasets[dataset_name], row.keys(), row)\n",
    "            print(\"--------------------------------\")\n",
    "        except Exception as e:\n",
    "            print(f\"{dataset_name}: {e}\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_prefix(examples):\n",
    "    examples[\"prefixed_text\"] = examples[\"text\"]\n",
    "    return examples\n",
    "mapped_datasets = datasets[\"DM_Mathematics\"].map(add_prefix, batched=True, remove_columns=[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#coding utf-8\n",
      "'''\n",
      "斐波那契数列-循环法\n",
      "'''\n",
      "def Fib_circle():\n",
      "    while True:   # 去掉while循环，只用for循环\n",
      "        num_1 = 0\n",
      "        num_2 = 1\n",
      "        fib_array = [0] # 用于存储计算出的FB数列值\n",
      "        m = input('你想要查找的起始项：')\n",
      "        n = input('你想要查找的结束项：')\n",
      "        if m.isdigit() and n.isdigit():   # 在这个实现函数中，不要进行检验。每个函数只做一个事情\n",
      "            m = int(m) # 将输入化为整数型\n",
      "            n = int(n)\n",
      "            for i in range(n):\n",
      "                num_1, num_2 = num_2, num_1 + num_2\n",
      "                fib_array.append(num_1)\n",
      "            print(f'你要查找的数列为{list(enumerate(fib_array[m:], m))}')\n",
      "            break\n",
      "        else:\n",
      "            print('请输入有效的正整数')\n",
      "\n",
      "if __name__ == '__main__':\n",
      "    Fib_circle()\n",
      "\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "# -*-coding: UTF-8 -*-\n",
      "#笨办法学编程py3-输入\n",
      "\n",
      "print(\"How old are you?\",end=\"\")\n",
      "\n",
      "age = input()\n",
      "\n",
      "print(\"How tall are you?\",end=\"\")\n",
      "\n",
      "height = input()\n",
      "\n",
      "print(\"How much do you weight?\",end=\"\") \n",
      "\n",
      "weight = input()\n",
      "\n",
      "print(\"So you're %r old,%r tall and %r heavry.\" %(age,height,weight))\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "#From stephen.marquard@uct.ac.za Sat Jan  5 09:14:16 2008\n",
      "#You will parse the From line using split() and\n",
      "# print out the second word in the line(i.e. the entire address of the person).\n",
      "#Then print out a count at the end.\n",
      "\n",
      "fname = input(\"Enter file name: \")\n",
      "if len(fname) < 1 : fname = \"mbox-short.txt\"\n",
      "\n",
      "fh = open(fname)\n",
      "count = 0\n",
      "\n",
      "for line in fh:\n",
      "    line = line.rstrip()\n",
      "    if not line.startswith('From:'): continue\n",
      "    pieces = line.split()\n",
      "    count = count + 1\n",
      "    print(pieces[1])\n",
      "\n",
      "print(\"There were\", count, \"lines in the file with From as the first word\")\n",
      "\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "import cv2\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "# function to run the canny \n",
      "def canny(image):\n",
      "\t# in - a new image\n",
      "\t# out - greyscael, smoothed, canny image. \n",
      "\t#greyscale the image\n",
      "\tgray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY) \n",
      "\t#blur the image\n",
      "\tblur = cv2.GaussianBlur(gray, (5,5),0)\n",
      "\t# canny - detect big changes in gradients using derivatives\n",
      "\tcanny = cv2.Canny(blur, 50, 150)\n",
      "\treturn canny\n",
      "\n",
      "\n",
      "def regionofinterest(image):\n",
      "\t# in - the image \n",
      "\t# out - a filled mask showing the lane we want \n",
      "\t# height of image = rows in the array\n",
      "\theight = image.shape[0]\n",
      "\t# actually just one but fillpoly dont like that\n",
      "\tpolygons = np.array([\n",
      "\t[(200,height), (1100, height), (550,250)]\n",
      "\t])\n",
      "\t# create an array of zeros same shape as image\n",
      "\tmask = np.zeros_like(image)\n",
      "\t# fill the region of interest with white only\n",
      "\tcv2.fillPoly(mask, polygons, 255)\n",
      "\t#use bitwise and to clear out uninteresting stuff\n",
      "\tmasked_image = cv2.bitwise_and(image,mask)\n",
      "\treturn masked_image\n",
      "\t\n",
      "def display_lines(image,lines):\n",
      "\tline_image = np.zeros_like(image)\n",
      "\tif lines is not None:\n",
      "\t\tfor line in lines:\n",
      "\t\t\tx1,y1,x2,y2 = line.reshape(4)\n",
      "\t\t\t# draw line blue and ten thick \n",
      "\t\t\tcv2.line(line_image, (x1,y1), (x2,y2),(255,0,0), 10)\n",
      "\treturn line_image\n",
      "\n",
      "def make_coordinates(image, line_parameters):\n",
      "\tslope, intercept = line_parameters\n",
      "\tprint (image.shape)\n",
      "\t# y's start at bottom x's use slope and intercet. \n",
      "\ty1 = image.shape[0]\n",
      "\t# 3/5 is an estimate \n",
      "\ty2 = int(y1 * 3/5)\n",
      "\tx1 = int((y1 - intercept)/slope)\n",
      "\tx2 = int((y2 - intercept)/slope)\n",
      "\treturn np.array([x1, y1, x2, y2])\n",
      "\n",
      "\t\n",
      "def average_slope_intercept(image,lines):\n",
      "\t# in - the original image and the lines image\n",
      "\n",
      "\tleft_fit =[]\n",
      "\tright_fit =[]\n",
      "\tfor line in lines:\n",
      "\t\tx1, y1, x2, y2 = line.reshape(4)\n",
      "\n",
      "\t\t# get the slope using a polynomial. \n",
      "\t\n",
      "\t\tparameters = np.polyfit((x1, x2), (y1,y2), 1)\n",
      "\t\tslope = parameters[0]\n",
      "\t\tintercept = parameters[1]\n",
      "\t\t# negative = for slope left hand side, positive is on right\n",
      "\t\tif slope < 0 :\n",
      "\t\t\tleft_fit.append((slope, intercept))\n",
      "\t\telse :\n",
      "\t\t\tright_fit.append((slope,intercept))\n",
      "\t\n",
      "\t# test the lines - print(left_fit)\n",
      "\t#print(right_fit)\n",
      "\t# average the sides and make sure to do along the right access(0)\n",
      "\t\n",
      "\tleft_fit_average = np.average(left_fit, axis = 0)\n",
      "\tright_fit_average = np.average(right_fit, axis = 0)\n",
      "\t\n",
      "\t\n",
      "\t#print(left_fit_average, 'left')\n",
      "\t#print(right_fit_average, 'right')\n",
      "\t\n",
      "\t# need to get the coordinates - created a function above\n",
      "\tleft_line = make_coordinates(image, left_fit_average)\n",
      "\tright_line = make_coordinates(image, right_fit_average)\n",
      "\t# return the lines\n",
      "\treturn np.array([left_line, right_line])\n",
      "\t\n",
      "\t\n",
      "\n",
      "# take in the video\n",
      "\n",
      "cap = cv2.VideoCapture(\"test2.mp4\")\n",
      "while(cap.isOpened()):\n",
      "\t_, frame = cap.read()\n",
      "\tcanny_image = canny(frame)\n",
      "\tcropped_image = regionofinterest(canny_image)\n",
      "\tlines = cv2.HoughLinesP(cropped_image, 2,np.pi/180, 100, np.array([]), minLineLength = 40, maxLineGap = 5)\n",
      "\taveraged_lines = average_slope_intercept(frame,lines)\n",
      "\tline_image = display_lines(frame, averaged_lines)\n",
      "\tcombo_image = cv2.addWeighted(frame, 0.8, line_image, 1,1)\n",
      "\tcv2.imshow('result', combo_image)\n",
      "\t#cv2.waitKey(1)\n",
      "\t# allow us to close the video - q\n",
      "\tif cv2.waitKey(1) == ord('q'):\n",
      "\t\tbreak\n",
      "cap.release()\n",
      "cv2.destroyAllWindows()\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "print(\"1\")\n",
      "\n",
      "list1 = [1,2,3,4]\n",
      "a = [5, 6, 7, 8]\n",
      "\n",
      "list1.append(a)\n",
      "print(list1)\n",
      "list1 = [1,2,3,4]\n",
      "list1.extend(a)\n",
      "print(list1)\n",
      "\n",
      "print(\"2\")\n",
      "\n",
      "list1 = [1,2,3,4]\n",
      "a = 'test'\n",
      "\n",
      "list1.append(a)\n",
      "print(list1)\n",
      "list1 = [1,2,3,4]\n",
      "list1.extend(a)\n",
      "print(list1)\n",
      "\n",
      "print(\"3\")\n",
      "\n",
      "list1 = [1,2,3,4]\n",
      "a = ['a', 'b', 'c']\n",
      "\n",
      "list1.append(a)\n",
      "print(list1)\n",
      "list1 = [1,2,3,4]\n",
      "list1.extend(a)\n",
      "print(list1)\n",
      "\n",
      "print(\"4\")\n",
      "\n",
      "list1 = [1,2,3,4]\n",
      "a = '阿貓'\n",
      "\n",
      "list1.append(a)\n",
      "print(list1)\n",
      "list1 = [1,2,3,4]\n",
      "list1.extend(a)\n",
      "print(list1)\n",
      "\n",
      "print(\"5\")\n",
      "\n",
      "list1 = [1,2,3,4]\n",
      "a = ['阿貓', '阿狗']\n",
      "\n",
      "list1.append(a)\n",
      "print(list1)\n",
      "list1 = [1,2,3,4]\n",
      "list1.extend(a)\n",
      "print(list1)\n",
      "\n",
      "print(\"6\")\n",
      "\n",
      "list1 = [1,2,3,4]\n",
      "a = 0\n",
      "\n",
      "list1.append(a)\n",
      "print(list1)\n",
      "list1 = [1,2,3,4]\n",
      "list1.extend(a)\n",
      "print(list1)\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "iterator = iter(datasets[\"python_edu\"])\n",
    "\n",
    "for i, row in enumerate(iterator):\n",
    "    print(row[\"text\"])\n",
    "    print(\"----\"*100)\n",
    "    if i > 3:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"json\", data_files=files, streaming=True, split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "IterableDataset({\n",
       "    features: Unknown,\n",
       "    n_shards: 63\n",
       "})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterator = iter(dataset)\n",
    "row = next(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#coding utf-8\n",
      "'''\n",
      "斐波那契数列-循环法\n",
      "'''\n",
      "def Fib_circle():\n",
      "    while True:   # 去掉while循环，只用for循环\n",
      "        num_1 = 0\n",
      "        num_2 = 1\n",
      "        fib_array = [0] # 用于存储计算出的FB数列值\n",
      "        m = input('你想要查找的起始项：')\n",
      "        n = input('你想要查找的结束项：')\n",
      "        if m.isdigit() and n.isdigit():   # 在这个实现函数中，不要进行检验。每个函数只做一个事情\n",
      "            m = int(m) # 将输入化为整数型\n",
      "            n = int(n)\n",
      "            for i in range(n):\n",
      "                num_1, num_2 = num_2, num_1 + num_2\n",
      "                fib_array.append(num_1)\n",
      "            print(f'你要查找的数列为{list(enumerate(fib_array[m:], m))}')\n",
      "            break\n",
      "        else:\n",
      "            print('请输入有效的正整数')\n",
      "\n",
      "if __name__ == '__main__':\n",
      "    Fib_circle()\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(row[\"text\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/haolin.chen/torchprime/venv/lib/python3.10/site-packages/torch_xla/__init__.py:258: UserWarning: `tensorflow` can conflict with `torch-xla`. Prefer `tensorflow-cpu` when using PyTorch/XLA. To silence this warning, `pip uninstall -y tensorflow && pip install tensorflow-cpu`. If you are in a notebook environment such as Colab or Kaggle, restart your notebook runtime afterwards.\n",
      "  warnings.warn(\n",
      "WARNING:root:libtpu.so and TPU device found. Setting PJRT_DEVICE=TPU.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"qwen/Qwen3-1.7B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Qwen2TokenizerFast(name_or_path='qwen/Qwen3-1.7B', vocab_size=151643, model_max_length=131072, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'eos_token': '<|im_end|>', 'pad_token': '<|endoftext|>', 'additional_special_tokens': ['<|im_start|>', '<|im_end|>', '<|object_ref_start|>', '<|object_ref_end|>', '<|box_start|>', '<|box_end|>', '<|quad_start|>', '<|quad_end|>', '<|vision_start|>', '<|vision_end|>', '<|vision_pad|>', '<|image_pad|>', '<|video_pad|>']}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n",
       "\t151643: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151644: AddedToken(\"<|im_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151645: AddedToken(\"<|im_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151646: AddedToken(\"<|object_ref_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151647: AddedToken(\"<|object_ref_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151648: AddedToken(\"<|box_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151649: AddedToken(\"<|box_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151650: AddedToken(\"<|quad_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151651: AddedToken(\"<|quad_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151652: AddedToken(\"<|vision_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151653: AddedToken(\"<|vision_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151654: AddedToken(\"<|vision_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151655: AddedToken(\"<|image_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151656: AddedToken(\"<|video_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151657: AddedToken(\"<tool_call>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151658: AddedToken(\"</tool_call>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151659: AddedToken(\"<|fim_prefix|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151660: AddedToken(\"<|fim_middle|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151661: AddedToken(\"<|fim_suffix|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151662: AddedToken(\"<|fim_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151663: AddedToken(\"<|repo_name|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151664: AddedToken(\"<|file_sep|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151665: AddedToken(\"<tool_response>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151666: AddedToken(\"</tool_response>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151667: AddedToken(\"<think>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151668: AddedToken(\"</think>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "}\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "164"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "with open(\"evaluations/loubnabnl/humaneval_infilling/flex-qwen3-1b-gcs-pretrain-all-data-dataloader-no-split-512_135000_20250630_065407.json\", \"r\") as f:\n",
    "    generations = json.load(f)\n",
    "\n",
    "len(generations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.add_column(\"completion\", generations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating json from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 185.48ba/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "442614"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.to_json(\"evaluations/loubnabnl/humaneval_infilling/flex-qwen3-1b-gcs-pretrain-all-data-dataloader-no-split-512_135000_20250630_065407.jsonl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
